


!pip install imblearn





import pandas as pd


dados = pd.read_csv('./Customer-Churn.csv')


dados.shape


dados.head()





#modificação de forma manual 
traducao_dic = {'Sim': 1, 
                'Nao': 0}

dadosmodificados = dados[['Conjuge', 'Dependentes', 'TelefoneFixo', 'PagamentoOnline', 'Churn']].replace(traducao_dic)
dadosmodificados.head()


#transformação pelo get_dummies
dummie_dados = pd.get_dummies(dados.drop(['Conjuge', 'Dependentes', 'TelefoneFixo', 'PagamentoOnline', 'Churn'],
                axis=1))

#junção dos dados trasformados com os que já tinhamos
dados_final = pd.concat([dadosmodificados, dummie_dados], axis=1)


dados_final.head()


dados.VariasLinhasTelefonicas.value_counts()





import seaborn as sns
%matplotlib inline
ax = sns.countplot(x='Churn', data=dados_final)


dados_final.Churn.value_counts()


from imblearn.over_sampling import SMOTE


X = dados_final.drop('Churn', axis = 1)
y = dados_final['Churn']


smt = SMOTE(random_state=123)
X, y = smt.fit_resample(X, y)


dados_final = pd.concat([X, y], axis=1)
dados_final.head()


ax = sns.countplot(x='Churn', data=dados_final)


dados_final.Churn.value_counts()





# Nova cliente
Xmaria = [[0,0,1,1,0,0,39.90,1,0,0,0,1,0,1,0,0,0,0,1,1,1,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1]]


X = dados_final.drop('Churn', axis = 1)
y = dados_final['Churn']


#biblioteca para padronizar os dados
from sklearn.preprocessing import StandardScaler


norm = StandardScaler()

X_normalizado = norm.fit_transform(X)
X_normalizado[0]


Xmaria_normalizado = norm.transform(pd.DataFrame(Xmaria, columns = X.columns))
Xmaria_normalizado





import numpy as np

# Calculando a distância da nova cliente com um dos clientes, esse processo será feito para todos os clientes
a = Xmaria_normalizado
b = X_normalizado[0]

np.sqrt(np.sum(np.square(a-b)))





#biblioteca para divisão dos dados
from sklearn.model_selection import train_test_split
X_treino, X_teste, y_treino, y_teste = train_test_split(X_normalizado, y, test_size=0.3, random_state=123)


#biblioteca para criarmos o modelo de machine learning
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(metric='euclidean')
knn.fit(X_treino, y_treino)
predito_knn = knn.predict(X_teste)
predito_knn





from sklearn.naive_bayes import BernoulliNB


X_treino


np.median(X_treino)


bnb = BernoulliNB()
bnb.fit(X_treino, y_treino)
predito_BNb = bnb.predict(X_teste)
predito_BNb





from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(criterion='entropy', random_state=42)
dtc.fit(X_treino, y_treino)


dtc.feature_importances_


predito_ArvoreDecisao = dtc.predict(X_teste)
predito_ArvoreDecisao











from sklearn.metrics import confusion_matrix


print(confusion_matrix(y_teste, predito_knn))


print(confusion_matrix(y_teste, predito_BNb))


print(confusion_matrix(y_teste, predito_ArvoreDecisao))





from sklearn.metrics import accuracy_score


#modelo KNN
print(accuracy_score(y_teste, predito_knn))


#modelo Bernoulli de naive bayes
print(accuracy_score(y_teste, predito_BNb))


#modelo árvore de decisão
print(accuracy_score(y_teste, predito_ArvoreDecisao))





from sklearn.metrics import precision_score


#modelo KNN
print(precision_score(y_teste, predito_knn))


#modelo Bernoulli de naive bayes
print(precision_score(y_teste, predito_BNb))


#modelo árvore de decisão
print(precision_score(y_teste, predito_ArvoreDecisao))





from sklearn.metrics import recall_score


#modelo KNN
print(recall_score(y_teste, predito_knn))


#modelo Bernoulli de naive bayes
print(recall_score(y_teste, predito_BNb))


#modelo árvore de decisão
print(recall_score(y_teste, predito_ArvoreDecisao))





print('Modelo KNN: ', recall_score(y_teste, predito_knn))
print('Modelo Bernoulli de Naive Bayes: ', recall_score(y_teste, predito_BNb))
print('Modelo Árvore de Decisão: ', recall_score(y_teste, predito_ArvoreDecisao))



